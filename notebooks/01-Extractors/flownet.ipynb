{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extract respiratory signals with FlowNet2 optical flow\n",
    "\n",
    "Based on \"FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\" is a deep learning model for optical flow estimation. The optical flow directions and magnitudes can be used to extract respiratory signals from videos. This notebook demonstrates how to use RAFT to extract respiratory signals from videos."
   ],
   "id": "e931c39f16bae87e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()\n",
    "\n",
    "subject = 'Proband16'\n",
    "setting = '101_natural_lighting'\n",
    "\n",
    "video_path = dataset.get_video_path(subject, setting)"
   ],
   "id": "3f5527a9bf000",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the FlowNet model",
   "id": "8b708a15f7bc46fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import respiration.utils as utils\n",
    "\n",
    "from respiration.extractor.flownet import (\n",
    "    FlowNet2,\n",
    "    resize_and_center_frames,\n",
    ")\n",
    "\n",
    "device = utils.get_torch_device()\n",
    "path = utils.file_path('data', 'flownet', 'FlowNet2_checkpoint.pth')\n",
    "loaded = torch.load(path)\n",
    "\n",
    "model = FlowNet2(batch_norm=False)\n",
    "model.load_state_dict(loaded['state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()"
   ],
   "id": "c50c750fcb5657b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extract optical flow from the video",
   "id": "b67acd67436fb434"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import respiration.utils as utils\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "param = utils.get_video_params(video_path)\n",
    "\n",
    "# Only get the first 12 seconds of the video\n",
    "param.num_frames = param.fps * 30\n",
    "\n",
    "# Number of frames that are processed at once\n",
    "batch_size = 10\n",
    "\n",
    "new_dim = 640\n",
    "\n",
    "# Store the optical flows vectors (N, 2, H, W)\n",
    "optical_flows = np.zeros((param.num_frames - 1, 2, new_dim, new_dim), dtype=np.float32)\n",
    "\n",
    "# Extract the optical flow from the video in batches\n",
    "for start in tqdm(range(0, param.num_frames, batch_size - 1)):\n",
    "    num_frames = min(batch_size, param.num_frames - start)\n",
    "    frames, _ = utils.read_video_rgb(video_path, num_frames, start)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(mode='RGB'),\n",
    "        # Center Crop the frames\n",
    "        transforms.CenterCrop((new_dim, new_dim)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    frames = torch.stack([preprocess(frame) for frame in frames], dim=0)\n",
    "    frames = frames.to(device)\n",
    "\n",
    "    # Fold the frames into (T, C, 2, H, W)\n",
    "    unfolded_frames = frames.unfold(0, 2, 1).permute(0, 1, 4, 2, 3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        flows = model(unfolded_frames)\n",
    "\n",
    "    # Garbage collect...\n",
    "    del frames, unfolded_frames\n",
    "\n",
    "    for idx in range(flows.shape[0]):\n",
    "        # Add the optical flow to the numpy array\n",
    "        optical_flows[start + idx] = flows[idx].cpu().numpy()"
   ],
   "id": "ef2039ce2b562fa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize the optical flow",
   "id": "21f8ecfff969c042"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "figure_dir = utils.dir_path('outputs', 'figures', 'flownet', mkdir=True)",
   "id": "73ab1e6b51bb90c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import respiration.extractor.optical_flow_raft as raft\n",
    "\n",
    "frames, _ = utils.read_video_rgb(video_path, 1, 1)\n",
    "frames = resize_and_center_frames(frames, new_dim)\n",
    "\n",
    "frame = frames[0].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Make frames ints 0..255\n",
    "frame = (frame * 255).astype(np.uint8)\n",
    "\n",
    "arrow_frame = raft.draw_flow(frame, optical_flows[0])\n",
    "flow_frame = raft.image_from_flow(optical_flows[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "ax[0].imshow(arrow_frame)\n",
    "ax[0].set_title('Optical flow arrows')\n",
    "ax[1].imshow(flow_frame)\n",
    "ax[1].set_title('Optical flow magnitude')\n",
    "\n",
    "utils.savefig(fig, figure_dir, 'optical_flow')"
   ],
   "id": "55a7216db8ee3b34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extract the respiratory signal\n",
    "\n",
    "1. Find the region of interest (ROI) on the chest\n",
    "2. Calculate the motion magnitude in the ROI\n",
    "3. Plot the motion magnitude over time"
   ],
   "id": "874aecbbb53b00ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.roi as roi\n",
    "\n",
    "# Find the chest region\n",
    "x, y, w, h = roi.detect_chest(frame)\n",
    "\n",
    "# Get only the optical flows in the chest region\n",
    "roi_flows = optical_flows[:, :, y:y + h, x:x + w]\n",
    "\n",
    "# Calculate motion magnitude by squaring the x and y components and taking the square root\n",
    "magnitudes = np.sqrt(roi_flows[:, 1] ** 2)"
   ],
   "id": "8aa5cd3d30abeb14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean_curve = np.mean(magnitudes, axis=(1, 2))\n",
    "std_curve = np.std(magnitudes, axis=(1, 2))"
   ],
   "id": "8d213c646c6e4318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
    "ax.plot(mean_curve, label='Mean')\n",
    "ax.fill_between(\n",
    "    np.arange(len(mean_curve)),\n",
    "    mean_curve - std_curve,\n",
    "    mean_curve + std_curve,\n",
    "    alpha=0.3,\n",
    "    label='Standard deviation')\n",
    "\n",
    "ax.set_title('Motion magnitude in the ROI')\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Motion magnitude')\n",
    "\n",
    "utils.savefig(fig, figure_dir, 'roi_magnitudes')"
   ],
   "id": "3f348a0c6c9a6316",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.analysis as analysis\n",
    "\n",
    "respiratory_gt = dataset.get_breathing_signal(subject, setting)[1:param.num_frames]\n",
    "\n",
    "comparator = analysis.SignalComparator(respiratory_gt, mean_curve, sample_rate=param.fps)\n",
    "utils.pretty_print(comparator.errors())"
   ],
   "id": "c22b159aeeca2805",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
    "ax.plot(comparator.prediction, label='Prediction')\n",
    "ax.plot(comparator.ground_truth, label='Ground truth')\n",
    "ax.set_title('Respiratory signal')\n",
    "ax.set_xlabel('Frame')\n",
    "ax.set_ylabel('Motion magnitude')\n",
    "ax.legend()"
   ],
   "id": "1b8b4fcc54ac5dad",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
