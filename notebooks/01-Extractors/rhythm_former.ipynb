{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Extract Respiration Signal with PhysFormer",
   "id": "52f6c1718f03adc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()\n",
    "\n",
    "subject = 'Proband21'\n",
    "scenario = '101_natural_lighting'"
   ],
   "id": "4b78c47457679ff0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "frames, meta = dataset.get_video_rgb(\n",
    "    subject,\n",
    "    scenario,\n",
    "    num_frames=30 * 20,\n",
    "    show_progress=True,\n",
    ")"
   ],
   "id": "a61011c940742495",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gt_respiration = dataset.get_breathing_signal(subject, scenario)\n",
    "\n",
    "# Cut the signal to match the number of frames\n",
    "gt_respiration = gt_respiration[:len(frames)]"
   ],
   "id": "e27f2bcc0fe12040",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import respiration.utils as utils\n",
    "from respiration.extractor.rhythm_former import *\n",
    "\n",
    "device = utils.get_torch_device()\n",
    "\n",
    "# Pretrained PPG models\n",
    "# model_checkpoint = utils.file_path('data', 'rhythm_former', 'MMPD_intra_RhythmFormer.pth')\n",
    "# model_checkpoint = utils.file_path('data', 'rhythm_former', 'PURE_cross_RhythmFormer.pth')\n",
    "# model_checkpoint = utils.file_path('data', 'rhythm_former', 'UBFC_cross_RhythmFormer.pth')\n",
    "\n",
    "# Fine-tuned Respiration models\n",
    "# model_checkpoint = utils.file_path(\n",
    "#     'models', 'rhythm_former', '20240721_173436', 'RhythmFormer', 'RhythmFormer_4.pth')\n",
    "# model_checkpoint = utils.file_path(\n",
    "#     'models', 'rhythm_former', '20240721_181857', 'RhythmFormer', 'RhythmFormer_4.pth')\n",
    "# model_checkpoint = utils.file_path(\n",
    "#     'models', 'rhythm_former', '20240721_215042', 'RhythmFormer', 'RhythmFormer_6.pth')\n",
    "# model_checkpoint = utils.file_path(\n",
    "#     'models', 'rhythm_former', '20240721_185122', 'RhythmFormer', 'RhythmFormer_9.pth')\n",
    "# model_checkpoint = utils.file_path(\n",
    "#     'models', 'rhythm_former', '20240722_115720', 'RhythmFormer', 'RhythmFormer_1.pth')\n",
    "model_checkpoint = utils.file_path(\n",
    "    'models', 'rhythm_former', '20240722_185129', 'RhythmFormer', 'RhythmFormer_8.pth')\n",
    "\n",
    "model = RhythmFormer()\n",
    "# Fix model loading: Some key have an extra 'module.' prefix\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "key_matching = model.load_state_dict(torch.load(model_checkpoint, map_location=device))\n",
    "key_matching"
   ],
   "id": "c4dc5f528d1e36c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Preprocess the frames to be in 128x128 with torch\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(mode='RGB'),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Assuming `frames` is a list of frame images\n",
    "transformed_frames = [\n",
    "    transform(frame) for frame in frames\n",
    "]\n",
    "\n",
    "# Optionally, stack the list of transformed frames into a single tensor\n",
    "frames_torch = torch.stack(transformed_frames).to(device)\n",
    "\n",
    "# Create batches of size 20\n",
    "frames_torch = frames_torch.unsqueeze(0)\n",
    "frames_torch.shape"
   ],
   "id": "7d73550aebdd896",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(frames_torch.to(device))\n",
    "    print(output.shape)"
   ],
   "id": "751fbfa239173fca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the out and the ground truth on two separate plots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 5))\n",
    "\n",
    "axs[0].plot(output.cpu().numpy().flatten())\n",
    "axs[0].set_title('Prediction')\n",
    "\n",
    "axs[1].plot(gt_respiration)\n",
    "axs[1].set_title('Ground Truth')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f557fc08c7afabc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.analysis as analysis\n",
    "\n",
    "output_processed = analysis.butterworth_filter(output.cpu().numpy().flatten(), 30, 0.08, 0.6)\n",
    "\n",
    "# Plot the out and the ground truth on two separate plots\n",
    "_, axs = plt.subplots(2, 1, figsize=(20, 5))\n",
    "\n",
    "axs[0].plot(output_processed)\n",
    "axs[0].set_title('Prediction')\n",
    "\n",
    "axs[1].plot(gt_respiration)\n",
    "axs[1].set_title('Ground Truth')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a9ff37df6f7845d6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
