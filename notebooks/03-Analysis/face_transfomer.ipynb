{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Face Transformer Analysis",
   "id": "ab8bdee78031cc65"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import respiration.utils as utils\n",
    "\n",
    "evaluation_dir = utils.dir_path('outputs', 'signals')\n",
    "\n",
    "predictions_file = utils.join_paths(evaluation_dir, 'transformer_predictions.csv')\n",
    "predictions = pd.read_csv(predictions_file)\n",
    "predictions['signal'] = predictions['signal'].apply(eval).apply(np.array)\n",
    "predictions.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compare the predictions to the ground truth",
   "id": "dac9f58f30a88636"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()"
   ],
   "id": "8ccf1401b5dd3fc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.analysis as analysis\n",
    "\n",
    "models = predictions['model'].unique()\n",
    "\n",
    "analysis_results = []\n",
    "\n",
    "for model in models:\n",
    "    signals = predictions[predictions['model'] == model]\n",
    "\n",
    "    experiment_analysis = analysis.Analysis()\n",
    "\n",
    "    for idx, row in signals.iterrows():\n",
    "        subject, setting = row['subject'], row['setting']\n",
    "        prediction = row['signal']\n",
    "        gt_signal = dataset.get_breathing_signal(subject, setting)\n",
    "        experiment_analysis.add_data(prediction, gt_signal, 30)\n",
    "\n",
    "    metrics = experiment_analysis.compute_metrics()\n",
    "    for entry in metrics:\n",
    "        analysis_results.append({\n",
    "            'model': model,\n",
    "            'metric': entry['metric'],\n",
    "            'method': entry['method'],\n",
    "            'value': entry['value'],\n",
    "        })\n",
    "\n",
    "analysis_results = pd.DataFrame(analysis_results)"
   ],
   "id": "6881d1ee98c555b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis_results",
   "id": "93095fc1f76f3f43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make all metrics positive\n",
    "analysis_results['value'] = analysis_results['value'].abs()\n",
    "\n",
    "# Invert the correlation metric --> Lower is now better\n",
    "for idx, row in analysis_results[analysis_results['metric'] == 'Correlation'].iterrows():\n",
    "    analysis_results.at[idx, 'value'] = 1 - row['value']"
   ],
   "id": "e74b91fba7c77e00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis_results",
   "id": "80415c26913e606b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Rank the models based on the metrics\n",
    "model_rankings = []\n",
    "\n",
    "for metric in analysis_results['metric'].unique():\n",
    "    for method in analysis_results['method'].unique():\n",
    "        metric_results = analysis_results[\n",
    "            (analysis_results['metric'] == metric) &\n",
    "            (analysis_results['method'] == method)]\n",
    "\n",
    "        ranks = metric_results.groupby('model')['value'].mean().sort_values().index\n",
    "        for idx, rank in enumerate(ranks):\n",
    "            model_rankings.append({\n",
    "                'model': rank,\n",
    "                'metric': metric,\n",
    "                'method': method,\n",
    "                'rank': idx + 1\n",
    "            })\n",
    "\n",
    "model_rankings = pd.DataFrame(model_rankings)"
   ],
   "id": "b99a814936e3e863",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_rankings",
   "id": "5da84141b3cac1c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Only keep the MAE and Correlation metrics\n",
    "model_rankings = model_rankings[model_rankings['metric'].isin(['MAE', 'Correlation'])]"
   ],
   "id": "68236b3477d09029",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Average the ranks for each model\n",
    "average_ranks = model_rankings.groupby('model')['rank'].mean().sort_values().reset_index()\n",
    "average_std = model_rankings.groupby('model')['rank'].std().sort_values().reset_index()\n",
    "\n",
    "# Merge the average ranks and standard deviations\n",
    "average_ranks = average_ranks.merge(average_std, on='model', suffixes=('_mean', '_std'))\n",
    "average_ranks"
   ],
   "id": "d738cb9d75c5bfd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Show some predictions",
   "id": "71c65bf82b5b6461"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()"
   ],
   "id": "c4f70f6b9113d3f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "subject = 'Proband25'\n",
    "setting = '303_normalized_face'\n",
    "\n",
    "gt_signal = dataset.get_breathing_signal(subject, setting)\n",
    "# gt_signal = analysis.butterworth_filter(gt_signal, 30, 0.08, 0.5)\n",
    "gt_signal = analysis.normalize_signal(gt_signal)\n",
    "\n",
    "prediction = predictions[\n",
    "    (predictions['model'] == '20240710_142159') &\n",
    "    (predictions['subject'] == subject) &\n",
    "    (predictions['setting'] == setting)]['signal'].values[0]\n",
    "# prediction = analysis.butterworth_filter(prediction, 30, 0.08, 0.5)\n",
    "prediction = analysis.normalize_signal(prediction)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(gt_signal, label='Ground Truth')\n",
    "plt.plot(prediction, label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "24803d98a26a147f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot all models for a single subject\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for model in models:\n",
    "    prediction = predictions[\n",
    "        (predictions['model'] == model) &\n",
    "        (predictions['subject'] == subject) &\n",
    "        (predictions['setting'] == setting)]['signal'].values[0]\n",
    "    prediction = analysis.normalize_signal(prediction)\n",
    "    plt.plot(prediction, label=model)\n",
    "\n",
    "plt.title('Predictions for Proband21')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "27a618d44e5a9feb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
