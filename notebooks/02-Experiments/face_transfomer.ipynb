{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extracting Respiration Signals with Face Transformer\n",
    "\n",
    "This notebook creates predictions of respiratory signals for all models trained with the face transformer architecture."
   ],
   "id": "344c1b3a21abce83"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import respiration.utils as utils\n",
    "\n",
    "model_ids = [\n",
    "    '20240710_194632',\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_size = 256\n",
    "device = utils.get_torch_device()"
   ],
   "id": "10c9d9e53a3e5a58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from vit_pytorch import SimpleViT\n",
    "\n",
    "\n",
    "def load_model(model_id: str) -> (SimpleViT, dict):\n",
    "    model_dir = utils.dir_path('models', 'transformer', model_id)\n",
    "    manifest_path = utils.join_paths(model_dir, 'manifest.json')\n",
    "    manifest = utils.read_json(manifest_path)\n",
    "\n",
    "    model = SimpleViT(\n",
    "        image_size=image_size,\n",
    "        patch_size=manifest['image_patch_size'],\n",
    "        num_classes=1,\n",
    "        dim=manifest['embedding_dim'],\n",
    "        heads=manifest['heads'],\n",
    "        mlp_dim=manifest['mlp_dim'],\n",
    "        depth=manifest['depth'],\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the best model from the training process\n",
    "    model_path = utils.join_paths(model_dir, manifest['trained_models'][-1]['model'])\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, manifest"
   ],
   "id": "b432d6405d662fba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "from respiration.dataset import ScenarioLoader\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for model_id in tqdm(model_ids):\n",
    "    model, manifest = load_model(model_id)\n",
    "    scenarios = manifest['testing_scenarios']\n",
    "\n",
    "    for inx, (subject, setting) in enumerate(scenarios):\n",
    "        print(f'Processing {subject} - {setting}')\n",
    "        loader = ScenarioLoader(subject, setting, manifest['num_frames'], device)\n",
    "\n",
    "        prediction = []\n",
    "\n",
    "        for (frames, gt_classes) in loader:\n",
    "            frames = utils.normalize_frames(frames)\n",
    "            # Disable gradient computation and reduce memory consumption.\n",
    "            with torch.no_grad():\n",
    "                outputs = model(frames).squeeze()\n",
    "            prediction.extend(outputs.tolist())\n",
    "\n",
    "        predictions.append({\n",
    "            'subject': subject,\n",
    "            'setting': setting,\n",
    "            'model': model_id,\n",
    "            'signal': prediction,\n",
    "        })"
   ],
   "id": "9cd0754594965f72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(predictions)\n",
    "\n",
    "output_dir = utils.dir_path('outputs', 'signals', mkdir=True)\n",
    "\n",
    "# Save the evaluation dataframe\n",
    "csv_path = utils.join_paths(output_dir, 'transformer_predictions.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "df.head()"
   ],
   "id": "7e3d941678abefce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate the Predictions",
   "id": "5d12bcc803bff208"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import respiration.analysis as analysis\n",
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()\n",
    "\n",
    "prediction = predictions[3]\n",
    "subject = prediction['subject']\n",
    "setting = prediction['setting']\n",
    "\n",
    "gt_signal = dataset.get_breathing_signal(subject, setting)\n",
    "prediction_signal = np.array(prediction['signal'])\n",
    "\n",
    "compare = analysis.SignalComparator(\n",
    "    prediction_signal,\n",
    "    gt_signal[:len(prediction_signal)],\n",
    "    30,\n",
    "    detrend_tarvainen=False,\n",
    "    filter_signal=True,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(compare.ground_truth, label='Ground Truth')\n",
    "plt.plot(compare.prediction, label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "fa1d21ea88a35ab7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "compare.errors()",
   "id": "d6d6f6977763761b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
