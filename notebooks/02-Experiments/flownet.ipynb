{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extract respiratory signals with FlowNet2.0\n",
    "\n",
    "Based on \"FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\" this notebook extracts respiratory signals from videos using the FlowNet2.0 model. The optical flows are extracted from the videos and the respiratory signals are calculated from the motion vectors."
   ],
   "id": "e931c39f16bae87e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()\n",
    "\n",
    "# The scenarios (subject, setting) to process\n",
    "scenarios = dataset.get_scenarios(['101_natural_lighting'])"
   ],
   "id": "3f5527a9bf000",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.utils as utils\n",
    "\n",
    "flows_dir = utils.dir_path('outputs', 'flownet_flows', mkdir=False)\n",
    "device = utils.get_torch_device()"
   ],
   "id": "ff44cce386692d39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "manifest = {\n",
    "    'timestamp_start': datetime.now(),\n",
    "    'scenarios': scenarios,\n",
    "    'device': device,\n",
    "    'flows': [],\n",
    "    'incomplete_rois': [],\n",
    "}"
   ],
   "id": "8bc94e838198c88d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1: Extract optical flows\n",
    "\n",
    "This part is heavy on computational resources and may take a long time to complete. The optical flows are extracted from the videos and saved to disk. The extracted optical flows are stored in the `outputs/flownet` directory."
   ],
   "id": "68dfab9c5a7fbad7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.extractor.flownet import (\n",
    "    FlowNet2,\n",
    "    FlowNet2C,\n",
    "    FlowNet2CS,\n",
    "    FlowNet2CSS,\n",
    "    FlowNet2S,\n",
    "    FlowNet2SD,\n",
    ")\n",
    "\n",
    "models = {\n",
    "    'FlowNet2': 'FlowNet2_checkpoint.pth',\n",
    "    'FlowNet2C': 'FlowNet2-C_checkpoint.pth',\n",
    "    'FlowNet2CS': 'FlowNet2-CS_checkpoint.pth',\n",
    "    'FlowNet2CSS': 'FlowNet2-CSS_checkpoint.pth',\n",
    "    'FlowNet2S': 'FlowNet2-S_checkpoint.pth',\n",
    "    'FlowNet2SD': 'FlowNet2-SD_checkpoint.pth',\n",
    "}"
   ],
   "id": "ce2b743bcb93292a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_model(name):\n",
    "    path = utils.file_path('data', 'flownet', models[name])\n",
    "    loaded = torch.load(path)\n",
    "\n",
    "    match name:\n",
    "        case 'FlowNet2':\n",
    "            model = FlowNet2()\n",
    "        case 'FlowNet2C':\n",
    "            model = FlowNet2C()\n",
    "        case 'FlowNet2CS':\n",
    "            model = FlowNet2CS()\n",
    "        case 'FlowNet2CSS':\n",
    "            model = FlowNet2CSS()\n",
    "        case 'FlowNet2S':\n",
    "            model = FlowNet2S()\n",
    "        case 'FlowNet2SD':\n",
    "            model = FlowNet2SD()\n",
    "        case _:\n",
    "            raise ValueError(f'Unknown model: {name}')\n",
    "\n",
    "    model.load_state_dict(loaded['state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ],
   "id": "f03bce0ad3486b36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from respiration.extractor.flownet import resize_and_center_frames\n",
    "\n",
    "# Number of frames that are processed at once\n",
    "batch_size = 10\n",
    "\n",
    "for (subject, setting) in tqdm(scenarios):\n",
    "    print(f'Processing {subject} - {setting}')\n",
    "\n",
    "    video_path = dataset.get_video_path(subject, setting)\n",
    "    param = utils.get_video_params(video_path)\n",
    "\n",
    "    if param.width < param.height:\n",
    "        dimensions = param.height\n",
    "    else:\n",
    "        dimensions = param.width\n",
    "\n",
    "    for model_name in models:\n",
    "        print(f'  Extracting optical flows with {model_name}...')\n",
    "        model = load_model(model_name)\n",
    "\n",
    "        # Store the optical flows vectors (N, 2, H, W). N is the number of frames \n",
    "        # in the video minus one, because we calculate the optical flow between consecutive frames.\n",
    "        optical_flows = np.zeros((param.num_frames - 1, 2, dimensions, dimensions), dtype=np.float32)\n",
    "\n",
    "        # Extract the optical flow from the video in batches\n",
    "        for start in range(0, param.num_frames, batch_size - 1):\n",
    "            num_frames = min(batch_size, param.num_frames - start)\n",
    "\n",
    "            frames, _ = utils.read_video_rgb(video_path, num_frames, start)\n",
    "            frames = resize_and_center_frames(frames, dimensions)\n",
    "            frames = frames.to(device)\n",
    "\n",
    "            # Fold the frames into (T, C, 2, H, W)\n",
    "            frames = frames.unfold(0, 2, 1).permute(0, 1, 4, 2, 3)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                flows = model(frames)\n",
    "\n",
    "            for idx in range(flows.shape[0]):\n",
    "                # Add the optical flow to the numpy array\n",
    "                optical_flows[start + idx] = flows[idx].cpu().numpy()\n",
    "\n",
    "            # Garbage collect...\n",
    "            del frames, flows\n",
    "\n",
    "        # Store the extracted signals\n",
    "        filename = f'{subject}_{setting}_{model_name}.npy'\n",
    "        flow_file = os.path.join(flows_dir, filename)\n",
    "        np.save(flow_file, optical_flows)\n",
    "\n",
    "        # Garbage collect the optical flows (8.2GB)\n",
    "        del optical_flows\n",
    "\n",
    "        manifest['flows'].append({\n",
    "            'subject': subject,\n",
    "            'setting': setting,\n",
    "            'model': model_name,\n",
    "            'filename': filename,\n",
    "        })"
   ],
   "id": "c54b105646357fb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "manifest['timestamp_finish'] = datetime.now()\n",
    "\n",
    "output_dir = utils.dir_path('outputs', 'signals', mkdir=True)\n",
    "manifest_file = utils.join_paths(output_dir, 'flownet_manifest.json')\n",
    "utils.write_json(manifest_file, manifest)"
   ],
   "id": "4ade1837050c82f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2: Export respiratory signals\n",
    "\n",
    "This part reads the extracted optical flows and calculates the respiratory signals. The respiratory signals are saved to a CSV file in the `outputs/signals` directory."
   ],
   "id": "71b4a5c313f5d9c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the manifest file\n",
    "manifest = utils.read_json(manifest_file)"
   ],
   "id": "4346256ca89d898a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import respiration.roi as roi\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "extracted_signals = []\n",
    "\n",
    "for (subject, setting) in tqdm(scenarios):\n",
    "    for model_name in models:\n",
    "        filename = f'{subject}_{setting}_{model_name}.npy'\n",
    "        flow_file = os.path.join(flows_dir, filename)\n",
    "        assert os.path.exists(flow_file), f'File not found: {flow_file}'\n",
    "\n",
    "        optical_flows = np.load(flow_file)\n",
    "\n",
    "        video_path = dataset.get_video_path(subject, setting)\n",
    "        params = utils.get_video_params(video_path)\n",
    "        first_frame = dataset.get_first_frame(subject, setting)\n",
    "        roi_areas = roi.get_roi_areas(first_frame)\n",
    "        if len(roi_areas) < 3:\n",
    "            print(f'Warning: only {len(roi_areas)} ROIs found for {subject} - {setting}')\n",
    "            manifest['incomplete_rois'].append({\n",
    "                'subject': subject,\n",
    "                'setting': setting,\n",
    "                'rois': [name for (_, name) in roi_areas],\n",
    "            })\n",
    "\n",
    "        for ((x, y, w, h), name) in roi_areas:\n",
    "            # Select the motion vectors in the region of interest (N, 2, H, W)\n",
    "            flow_region = optical_flows[:, :, y:y + h, x:x + w]\n",
    "\n",
    "            # Horizontal motion (N, H, W)\n",
    "            u = flow_region[:, 0, :, :]\n",
    "\n",
    "            # Vertical motion (N, H, W)\n",
    "            v = flow_region[:, 1, :, :]\n",
    "\n",
    "            # Calculate the magnitudes of the motion vectors (N, H, W)\n",
    "            magnitudes = np.sqrt(u ** 2 + v ** 2)\n",
    "\n",
    "            # Calculate the mean and standard deviation of the magnitudes\n",
    "            uv_mean_curve = np.mean(magnitudes, axis=(1, 2))\n",
    "            uv_std_curve = np.std(magnitudes, axis=(1, 2))\n",
    "\n",
    "            # Calculate the mean and standard deviation of the vertical motion\n",
    "            v_mean_curve = v.mean(axis=(1, 2))\n",
    "            v_std_curve = v.std(axis=(1, 2))\n",
    "\n",
    "            # Store the extracted signals\n",
    "            extracted_signals.append({\n",
    "                'subject': subject,\n",
    "                'setting': setting,\n",
    "                'model': model_name,\n",
    "                'roi': name,\n",
    "                'sampling_rate': params.fps,\n",
    "                'signal_uv': uv_mean_curve.tolist(),\n",
    "                'signal_uv_std': uv_std_curve.tolist(),\n",
    "                'signal_v': v_mean_curve.tolist(),\n",
    "                'signal_v_std': v_std_curve.tolist(),\n",
    "            })\n",
    "\n",
    "        del optical_flows"
   ],
   "id": "ed2b36e28722b16e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "signals_df = pd.DataFrame(extracted_signals)\n",
    "predictions_file = os.path.join(output_dir, 'flownet_predictions.csv')\n",
    "signals_df.to_csv(predictions_file, index=False)"
   ],
   "id": "9b4b207e7fe79198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "signals_df.head()",
   "id": "fa3b70a86faefa49",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
