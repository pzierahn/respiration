{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformer Classifier\n",
    "\n",
    "This notebook trains a Transformer based classifier to predict inhaling and exhaling from video frames."
   ],
   "id": "67dc06081e64a18c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import respiration.utils as utils\n",
    "\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "\n",
    "# The timestamp is the unique identifier for this training run\n",
    "zone = timezone('Europe/Berlin')\n",
    "model_id = datetime.now().astimezone(zone).strftime('%Y%m%d_%H%M%S')\n",
    "device = utils.get_torch_device()\n",
    "\n",
    "# The manifest will store all the metadata for this training run\n",
    "manifest = {\n",
    "    'id': model_id,\n",
    "    'device': str(device),\n",
    "    'timestamp_start': datetime.now().astimezone().isoformat(),\n",
    "    'dataset': 'VitalCamSet',\n",
    "}\n",
    "model_id"
   ],
   "id": "be65cae3ecad860b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "device",
   "id": "4df3cafeef133826",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define training and testing scenarios",
   "id": "a34c6df561579356"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from respiration.dataset import VitalCamSet\n",
    "\n",
    "dataset = VitalCamSet()\n",
    "scenarios_all = dataset.get_scenarios(['303_normalized_face'])\n",
    "\n",
    "split_ratio = 0.8\n",
    "manifest['split_ratio'] = split_ratio\n",
    "\n",
    "training = scenarios_all[:int(len(scenarios_all) * split_ratio)]\n",
    "manifest['training_scenarios'] = training\n",
    "\n",
    "testing = scenarios_all[int(len(scenarios_all) * split_ratio):]\n",
    "manifest['testing_scenarios'] = testing"
   ],
   "id": "a7a49d6cd0a17760",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_frames = 300\n",
    "manifest['num_frames'] = num_frames"
   ],
   "id": "92b4df33dcf3161",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define temporal shifting",
   "id": "6cf51af626d8711"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preprocess_frames(frames: torch.Tensor) -> torch.Tensor:\n",
    "    # Normalize the frames\n",
    "    frames = (frames - frames.min()) / (frames.max() - frames.min())\n",
    "    return frames"
   ],
   "id": "d84718446ec15852",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_size = 256\n",
    "manifest['image_size'] = image_size"
   ],
   "id": "6045eb1b0069b2bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "import respiration.utils as utils\n",
    "\n",
    "\n",
    "class ScenarioLoaderChunks:\n",
    "    \"\"\"\n",
    "    A data loader for the VitalCamSet dataset. This class loads the video frames and the ground truth signal for a\n",
    "    specific scenario. The video frames are loaded in chunks of a specific size. The ground truth signal is down-sampled\n",
    "    to match the video frames' dimensions.\n",
    "    \"\"\"\n",
    "    subject: str\n",
    "    setting: str\n",
    "    frames_per_segment: int\n",
    "\n",
    "    def __init__(self,\n",
    "                 subject: str,\n",
    "                 setting: str,\n",
    "                 frames_per_segment: int = num_frames):\n",
    "        self.subject = subject\n",
    "        self.setting = setting\n",
    "        self.frames_per_segment = frames_per_segment\n",
    "\n",
    "        self.video_path = dataset.get_video_path(subject, setting)\n",
    "        self.total_frames = utils.get_frame_count(self.video_path)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return math.ceil(self.total_frames / self.frames_per_segment)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= self.__len__():\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            item = self.__getitem__(self.current_index)\n",
    "            self.current_index += 1\n",
    "            return item\n",
    "\n",
    "    def __getitem__(self, index) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        Return the frames and the ground truth signal for the given index\n",
    "        :param index: The index of the chunk\n",
    "        :return: The frames and the ground truth signal\n",
    "        \"\"\"\n",
    "\n",
    "        if index >= self.__len__():\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "        start = index * self.frames_per_segment\n",
    "        end = start + self.frames_per_segment\n",
    "        size = min(self.frames_per_segment, self.total_frames - start)\n",
    "\n",
    "        # Load the video frames\n",
    "        frames, meta = utils.read_video_rgb(self.video_path, size, start)\n",
    "        frames = torch.tensor(frames, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Permute the dimensions to match the expected input format (B, C, H, W)\n",
    "        frames = frames.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Get the ground truth signal for the scenario\n",
    "        gt_waveform = dataset.get_breathing_signal(self.subject, self.setting)\n",
    "        gt_waveform = torch.tensor(gt_waveform.copy(), dtype=torch.float32, device=device)\n",
    "\n",
    "        #\n",
    "        # Normalize the signals: This normalizes the signal between -0.5 and 0.5. The values are based on the\n",
    "        # overall maximum and minimum values in the dataset.\n",
    "        #\n",
    "        \n",
    "        gt_overall_max, gt_overall_min = 6680.352219172085, -6572.075174276201\n",
    "        gt_waveform = (gt_waveform - gt_waveform.mean()) / (gt_overall_max - gt_overall_min)\n",
    "        gt_waveform = gt_waveform[start:end]\n",
    "\n",
    "        return frames, gt_waveform"
   ],
   "id": "3ec25f327cb4b1c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test the ScenarioLoader",
   "id": "157cbbbee85c69ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loader = ScenarioLoaderChunks('Proband22', '303_normalized_face')\n",
    "\n",
    "chunk_frames, chunk_signal = loader[4]\n",
    "chunk_frames.shape, chunk_signal.shape"
   ],
   "id": "39481a906bbf2d91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Training",
   "id": "1cccdc3abd23e9a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = utils.dir_path('outputs', 'logs', model_id, mkdir=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ],
   "id": "8c815980f7431cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from vit_pytorch import SimpleViT\n",
    "\n",
    "image_patch_size = 16\n",
    "manifest['image_patch_size'] = image_patch_size\n",
    "\n",
    "depth = 6\n",
    "manifest['depth'] = depth\n",
    "\n",
    "heads = 16\n",
    "manifest['heads'] = heads\n",
    "\n",
    "mlp_dim = 2048\n",
    "manifest['mlp_dim'] = mlp_dim\n",
    "\n",
    "embedding_dim = 512\n",
    "manifest['embedding_dim'] = embedding_dim\n",
    "\n",
    "num_classes = 1\n",
    "manifest['num_classes'] = num_classes\n",
    "\n",
    "model = SimpleViT(\n",
    "    image_size=image_size,\n",
    "    patch_size=image_patch_size,\n",
    "    num_classes=num_classes,\n",
    "    dim=embedding_dim,\n",
    "    heads=heads,\n",
    "    mlp_dim=mlp_dim,\n",
    "    depth=depth,\n",
    ").to(device)\n",
    "manifest['base_model'] = 'simple_vit'"
   ],
   "id": "bcc71511f5183e9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs = 30\n",
    "manifest['epochs'] = epochs\n",
    "\n",
    "learning_rate = 0.000001\n",
    "manifest['learning_rate'] = learning_rate\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "manifest['loss_fn'] = 'MSELoss'\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "manifest['optimizer'] = 'AdamW'"
   ],
   "id": "1f16e5efdceab177",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_one_epoch(epoch_index: int):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Iterate over the training scenarios\n",
    "    for (subject, setting) in training:\n",
    "        loader = ScenarioLoaderChunks(subject, setting)\n",
    "\n",
    "        scenario_loss = 0.0\n",
    "\n",
    "        # Iterate over the hole scenario video in chunks\n",
    "        for idy, (frames, gt_classes) in enumerate(loader):\n",
    "            frames = preprocess_frames(frames)\n",
    "\n",
    "            # Make predictions for this chunk\n",
    "            outputs = model(frames).squeeze()\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(outputs, gt_classes)\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gather data and report\n",
    "            print(f'  {subject} #{idy:02d} loss={loss.item()}')\n",
    "            scenario_loss += loss.item()\n",
    "\n",
    "        scenario_loss /= len(loader)\n",
    "        epoch_loss += scenario_loss\n",
    "\n",
    "        print(f'  >> {subject} scenario_loss={scenario_loss}')\n",
    "        writer.add_scalars('Training_Loss', {\n",
    "            f'{subject}_{setting}': scenario_loss,\n",
    "        }, epoch_index)\n",
    "        writer.flush()\n",
    "\n",
    "    return epoch_loss / len(training)"
   ],
   "id": "c5f12b4ed685e672",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_dir = utils.dir_path('models', 'transformer', model_id, mkdir=True)",
   "id": "bddc7e7fbb29988",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models = []\n",
    "best_loss = float('inf')"
   ],
   "id": "caf9abb654eebf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_manifest():\n",
    "    manifest['trained_models'] = models\n",
    "    manifest['best_testing_loss'] = float(best_loss)\n",
    "    manifest['timestamp_finish'] = datetime.now().astimezone().isoformat()\n",
    "    utils.write_json(os.path.join(model_dir, 'manifest.json'), manifest)"
   ],
   "id": "7e66bf39b3816485",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f'Epoch {epoch}:')\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for inx, (subject, setting) in enumerate(testing):\n",
    "            loader = ScenarioLoaderChunks(subject, setting)\n",
    "            testing_loss = 0.0\n",
    "\n",
    "            for (frames, gt_classes) in loader:\n",
    "                frames = preprocess_frames(frames)\n",
    "                voutputs = model(frames).squeeze()\n",
    "                testing_loss += torch.nn.functional.mse_loss(voutputs, gt_classes).item()\n",
    "\n",
    "            testing_loss /= len(loader)\n",
    "            writer.add_scalars('Testing_Loss', {f'{subject}_{setting}': testing_loss}, epoch)\n",
    "            print(f'  >> {subject} loss={testing_loss}')\n",
    "\n",
    "            running_loss += testing_loss\n",
    "\n",
    "    testing_loss = running_loss / len(testing)\n",
    "    print(f'LOSS training={avg_loss} testing={testing_loss}')\n",
    "    writer.add_scalars('Average_Loss', {\n",
    "        'Training': avg_loss,\n",
    "        'Testing': testing_loss,\n",
    "    }, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track the best performance, and save the model's state\n",
    "    if testing_loss < best_loss:\n",
    "        best_loss = testing_loss\n",
    "        model_name = f'{model_id}_{epoch}.pth'\n",
    "\n",
    "        model_path = os.path.join(model_dir, model_name)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        models.append({\n",
    "            'model': model_name,\n",
    "            'epoch': epoch,\n",
    "            'validation_loss': float(testing_loss),\n",
    "        })\n",
    "        save_manifest()"
   ],
   "id": "63f1645676dd1690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "save_manifest()",
   "id": "7c44122f4d4d4657",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
